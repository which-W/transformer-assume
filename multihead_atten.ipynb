{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198548f-bc0f-4a62-9f2b-f95dc170315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "输入emb后的词序列,根据Q,K,V方法计算词与词之间的相关性,为每个词生成信息提取后的emb(与输入词1:1映射)\n",
    "'''\n",
    "from torch import nn \n",
    "import torch \n",
    "from dataset import de_vocab,de_preprocess,train_dataset\n",
    "from emb import EmbeddingWithPosition\n",
    "import math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe872880-d50a-4217-a3d4-088682cfc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,emb_size,q_k_size,v_size,head):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.q_k_size = q_k_size\n",
    "        self.v_size = v_size\n",
    "        self.head = head\n",
    "\n",
    "        self.w_q = nn.Linear(emb_size,head*q_k_size) # 多头\n",
    "        self.w_k = nn.Linear(emb_size,head*q_k_size)\n",
    "        self.w_v = nn.Linear(emb_size,head*v_size)\n",
    "          # kvcache推理优化\n",
    "        self.kv_cache={}\n",
    "        self.kv_cache_type=''\n",
    "\n",
    "    def set_kvcache(self,kv_cache_type=''):\n",
    "        self.kv_cache_type=kv_cache_type\n",
    "        self.kv_cache={}\n",
    "        \n",
    "    def forward(self,x_q,x_k_v,attn_mask):\n",
    "        # kvcache推理加速,只有decoder推理阶段使用\n",
    "        if self.kv_cache_type=='selfattn': # decoder的自注意力cache，第一个mult_atten\n",
    "            x_q=x_q[:,-1:,:] # (batch_size,seq_len=1,emb_size)\n",
    "            x_k_v=x_k_v[:,-1:,:] # (batch_size,seq_len'=1,emb_size)\n",
    "            \n",
    "            q=self.w_q(x_q) # q: (batch_size,seq_len=1,head*q_k_size)\n",
    "            k=self.w_k(x_k_v) # k: (batch_size,seq_len=1,head*q_k_size)\n",
    "            v=self.w_v(x_k_v) # v: (batch_size,seq_len=1,head*v_size)\n",
    "            if 'Q' in self.kv_cache:\n",
    "                q=torch.concat((self.kv_cache['Q'],q),dim=1) # 追加到前一次推理的Q末尾\n",
    "            if 'K' in self.kv_cache:\n",
    "                k=torch.concat((self.kv_cache['K'],k),dim=1) # 追加到前一次推理的K末尾\n",
    "            if 'V' in self.kv_cache:\n",
    "                v=torch.concat((self.kv_cache['V'],v),dim=1) # 追加到前一次推理的V末尾\n",
    "            self.kv_cache.update({'Q':q.detach(),'K':k.detach(),'V':v.detach()}) # 更新缓存\n",
    "            \n",
    "        elif self.kv_cache_type=='crossattn': # decoder的交叉注意力cache,第二个mult_atten\n",
    "            x_q=x_q[:,-1:,:] # (batch_size,seq_len=1,emb_size)\n",
    "            q=self.w_q(x_q) # q: (batch_size,seq_len,head*q_k_size)\n",
    "            if 'Q' in self.kv_cache:\n",
    "                q=torch.concat((self.kv_cache['Q'],q),dim=1) # 追加到前一次推理的Q末尾\n",
    "            if 'K' in self.kv_cache:\n",
    "                k=self.kv_cache['K']\n",
    "            else:\n",
    "                k=self.w_k(x_k_v) # k: (batch_size,seq_len,head*q_k_size)\n",
    "            if 'V' in self.kv_cache:\n",
    "                v=self.kv_cache['V']\n",
    "            else:\n",
    "                v=self.w_v(x_k_v) # v: (batch_size,seq_len,head*v_size)\n",
    "            self.kv_cache.update({'Q':q.detach(),'K':k.detach(),'V':v.detach()}) # 更新缓存\n",
    "            \n",
    "        else: # 训练模式\n",
    "            q=self.w_q(x_q) # q: (batch_size,seq_len,head*q_k_size)\n",
    "            k=self.w_k(x_k_v) # k: (batch_size,seq_len,head*q_k_size)\n",
    "            v=self.w_v(x_k_v) # v: (batch_size,seq_len,head*v_size)\n",
    "\n",
    "        #多头的兼容与处理\n",
    "        q=q.view(q.size()[0],q.size()[1],self.head,self.q_k_size).transpose(1,2) # q: (batch_size,head,seq_len,q_k_size)\n",
    "        k=k.view(k.size()[0],k.size()[1],self.head,self.q_k_size).transpose(1,2).transpose(2,3) # k:(batch_size,head,q_k_size,seq_len)\n",
    "\n",
    "        # 形成注意力矩阵\n",
    "        attn=torch.matmul(q,k)/math.sqrt(self.q_k_size) # (batch_size,head,seq_len,seq_len) row是q,col是k 这里的除以q_k_size是为了减小点积方差进行缩放\n",
    "\n",
    "        # 注意力分值处理\n",
    "        # attn_mask: (batch_size,seq_len,seq_len) \n",
    "        attn_mask=attn_mask.unsqueeze(1).expand(-1,self.head,-1,-1) # attn_mask: (batch_size,head,seq_len,seq_len) 创建atten_mask掩码矩阵\n",
    "        attn_mask = attn_mask.bool()\n",
    "        attn=attn.masked_fill(attn_mask,-1e9) #满足mask掩码的设置一个极大负数\n",
    "        attn=torch.softmax(attn,dim=-1) # scores: (batch_size,head,seq_len,seq_len) #进行归一化处理，将极大负数归零且其他值化归到0~1\n",
    "\n",
    "         # 注意力与V相乘\n",
    "        v=v.view(v.size()[0],v.size()[1],self.head,self.v_size).transpose(1,2) # v: (batch_size,head,seq_len,v_size)\n",
    "        z=torch.matmul(attn,v) # z: (batch_size,head,seq_len,v_size)\n",
    "        z=z.transpose(1,2) # z: (batch_size,seq_len,head,v_size)\n",
    "        return z.reshape(z.size()[0],z.size()[1],-1) # z: (batch_size,seq_len,head*v_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fd363-1ed7-4581-af07-453406926049",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    # 准备1个batch\n",
    "    emb=EmbeddingWithPosition(len(de_vocab),128)\n",
    "    de_tokens,de_ids=de_preprocess(train_dataset[0][0]) # 从第一个样本中取一个语句对，再取de句子转词ID序列\n",
    "    de_ids_tensor=torch.tensor(de_ids,dtype=torch.long)\n",
    "    emb_result=emb(de_ids_tensor.unsqueeze(0)) # 加入batch维度，转batch后再输入模型\n",
    "    print('emb_result:', emb_result.size())\n",
    "\n",
    "    # 多头注意力\n",
    "    multihead=MultiHeadAttention(emb_size=128,q_k_size=256,v_size=512,head=8)\n",
    "    attn_mask=torch.zeros((1,de_ids_tensor.size()[0],de_ids_tensor.size()[0])) # batch中每个样本对应1个注意力矩阵\n",
    "    multihead_result=multihead(x_q=emb_result,x_k_v=emb_result,attn_mask=attn_mask)\n",
    "    print('multihead_result:', multihead_result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c0b8d-2ff0-4acb-bbf8-732b90454eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
