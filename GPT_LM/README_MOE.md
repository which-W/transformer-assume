# ä¼ä¸šçº§ MoE Transformer å®ç°

è¿™æ˜¯ä¸€ä¸ªå‚è€ƒ DeepSeek V2 è®¾è®¡ç†å¿µçš„ Mixture of Experts (MoE) Transformer å®ç°ã€‚

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

### 1. **å®Œæ•´çš„ MoE å®ç°**
- Top-K è·¯ç”±æœºåˆ¶ (é€šå¸¸ K=2)
- è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±
- å¤šä¸“å®¶å¹¶è¡Œè®¡ç®—
- é—¨æ§ç½‘ç»œ (Gating Network)

### 2. **çµæ´»çš„æ¶æ„é€‰æ‹©**
- **å…¨MoEæ¨¡å‹**: æ‰€æœ‰å±‚éƒ½ä½¿ç”¨ MoE
- **æ··åˆæ¨¡å‹**: éƒ¨åˆ†å±‚ç”¨ MoE,éƒ¨åˆ†å±‚ç”¨æ ‡å‡† FFN
- **å¯é…ç½®ä¸“å®¶æ•°**: æ”¯æŒ 4/8/16/32/64 ç­‰ä»»æ„ä¸“å®¶æ•°é‡

### 3. **è®­ç»ƒä¼˜åŒ–**
- è´Ÿè½½å‡è¡¡æŸå¤±é˜²æ­¢ä¸“å®¶å´©å¡Œ
- é—¨æ§æƒé‡å½’ä¸€åŒ–
- æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ (å¯é€‰)
- è¾…åŠ©æŸå¤±è‡ªåŠ¨èšåˆ

### 4. **ç”Ÿäº§å°±ç»ª**
- ç±»å‹æç¤ºå®Œæ•´
- è¯¦ç»†çš„æ–‡æ¡£æ³¨é‡Š
- æ¨¡å—åŒ–è®¾è®¡,æ˜“äºæ‰©å±•
- å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹

### MoE Block ç»“æ„

```
è¾“å…¥ x [batch, seq, d_model]
    â†“
LayerNorm
    â†“
Multi-Head Attention
    â†“
æ®‹å·®è¿æ¥ (+)
    â†“
LayerNorm
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MoE Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  â”‚
â”‚  Router (é—¨æ§ç½‘ç»œ)               â”‚
â”‚    â†“                             â”‚
â”‚  Top-K Selection                 â”‚
â”‚    â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Expertâ”‚Expertâ”‚ ... â”‚Expertâ”‚  â”‚
â”‚  â”‚  1   â”‚  2   â”‚     â”‚  N   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚    â†“                             â”‚
â”‚  åŠ æƒç»„åˆ (æŒ‰é—¨æ§æƒé‡)           â”‚
â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
æ®‹å·®è¿æ¥ (+)
    â†“
è¾“å‡º
```

### å…³é”®ç»„ä»¶

1. **MoERouter** (`moe_router.py`)
   - è®¡ç®—æ¯ä¸ª token å¯¹æ‰€æœ‰ä¸“å®¶çš„äº²å’Œåº¦
   - é€‰æ‹© Top-K ä¸ªæœ€ç›¸å…³çš„ä¸“å®¶
   - è®¡ç®—é—¨æ§æƒé‡
   - è´Ÿè½½å‡è¡¡æŸå¤±

2. **MoEExperts** (`moe_experts.py`)
   - åŒ…å« N ä¸ªç‹¬ç«‹çš„ä¸“å®¶ç½‘ç»œ
   - æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ª SwiGLU FFN
   - å¹¶è¡Œå¤„ç†åˆ†é…çš„ tokens

3. **MoELayer** (`moe_layer.py`)
   - æ•´åˆ Router å’Œ Experts
   - å¤„ç† token åˆ°ä¸“å®¶çš„åˆ†å‘
   - åŠ æƒç»„åˆä¸“å®¶è¾“å‡º

4. **MoETransformerBlock** (`moe_transformer_block.py`)
   - æ ‡å‡† Attention + MoE FFN
   - Pre-norm æ¶æ„
   - RMSNorm å½’ä¸€åŒ–

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `moe_router.py` | é—¨æ§è·¯ç”±å™¨,è´Ÿè´£ token-expert åŒ¹é… |
| `moe_experts.py` | ä¸“å®¶ç½‘ç»œé›†åˆ |
| `moe_layer.py` | å®Œæ•´çš„ MoE å±‚å®ç° |
| `moe_transformer_block.py` | MoE Transformer Block |
| `moe_transformer.py` | å®Œæ•´çš„ MoE è¯­è¨€æ¨¡å‹ |
| `moe_examples.py` | å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ |

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ MoE æ¨¡å‹

```python
from moe_transformer import MoETransformerLM

# åˆ›å»ºæ¨¡å‹
model = MoETransformerLM(
    d_model=512,
    n_head=8,
    vocab_size=10000,
    max_seq_len=1024,
    d_ff=2048,
    theta=10000.0,
    n_layer=12,
    n_experts=8,      # æ¯å±‚8ä¸ªä¸“å®¶
    top_k=2,          # æ¯ä¸ªtokenæ¿€æ´»2ä¸ªä¸“å®¶
    use_moe_aux_loss=True,
    moe_aux_loss_weight=0.01,
)

# å‰å‘ä¼ æ’­
import torch
token_ids = torch.randint(0, 10000, (4, 128))  # [batch, seq]
logits = model(token_ids)  # [batch, seq, vocab]
```

### 2. æ··åˆæ¶æ„æ¨¡å‹

```python
from moe_transformer import HybridMoETransformerLM

# ä»…åœ¨æŒ‡å®šå±‚ä½¿ç”¨ MoE
model = HybridMoETransformerLM(
    d_model=512,
    n_head=8,
    vocab_size=10000,
    max_seq_len=1024,
    d_ff=2048,
    theta=10000.0,
    n_layer=12,
    moe_layer_indices=[2, 5, 8, 11],  # ä»…è¿™4å±‚ç”¨MoE
    n_experts=8,
    top_k=2,
)
```

### 3. è®­ç»ƒå¾ªç¯

```python
model.train()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

for batch in dataloader:
    # å‰å‘ä¼ æ’­
    logits = model(batch['input_ids'])
    
    # è¯­è¨€æ¨¡å‹æŸå¤±
    lm_loss = F.cross_entropy(
        logits.view(-1, vocab_size),
        batch['labels'].view(-1)
    )
    
    # MoE è¾…åŠ©æŸå¤±
    aux_loss = model.get_aux_loss()
    
    # æ€»æŸå¤±
    total_loss = lm_loss + aux_loss
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

## ğŸ“– è¯¦ç»†è¯´æ˜

### MoE æ ¸å¿ƒæ¦‚å¿µ

#### 2. Top-K è·¯ç”±

```
ä¾‹å¦‚: 8 ä¸ªä¸“å®¶,Top-2 è·¯ç”±

Token: "artificial"
    â†“
Router è®¡ç®—å¾—åˆ†: [0.3, 0.1, 0.05, 0.25, 0.08, 0.12, 0.05, 0.05]
    â†“
é€‰æ‹© Top-2: Expert 1 (0.3), Expert 4 (0.25)
    â†“
å½’ä¸€åŒ–æƒé‡: [0.545, 0.455]
    â†“
è¾“å‡º = 0.545 * Expert1(token) + 0.455 * Expert4(token)
```

#### 3. è´Ÿè½½å‡è¡¡

**é—®é¢˜**: è®­ç»ƒæ—¶æŸäº›ä¸“å®¶å¯èƒ½è¢«è¿‡åº¦ä½¿ç”¨,å…¶ä»–ä¸“å®¶æ¬ ä½¿ç”¨

**è§£å†³æ–¹æ¡ˆ**: è¾…åŠ©æŸå¤±æƒ©ç½šä¸å‡åŒ€åˆ†å¸ƒ

```python
# è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡
expert_usage = [0.4, 0.3, 0.1, 0.05, 0.05, 0.05, 0.03, 0.02]

# ç†æƒ³æƒ…å†µ: æ¯ä¸ªä¸“å®¶ 1/8 = 0.125
# è¾…åŠ©æŸå¤±æƒ©ç½šåç¦»ç†æƒ³å€¼çš„æƒ…å†µ
```

### é…ç½®å»ºè®®

| æ¨¡å‹è§„æ¨¡ | n_experts | top_k | d_ff | è¯´æ˜ |
|---------|-----------|-------|------|------|
| å°å‹ | 4 | 2 | 1024 | å¿«é€Ÿå®éªŒ |
| ä¸­å‹ | 8 | 2 | 2048 | å¹³è¡¡æ€§èƒ½ |
| å¤§å‹ | 16 | 2 | 4096 | é«˜æ€§èƒ½ |
| è¶…å¤§å‹ | 64 | 2-4 | 8192 | DeepSeekçº§åˆ« |

### è¶…å‚æ•°è°ƒä¼˜

```python
# è¾…åŠ©æŸå¤±æƒé‡
# - å¤ªå°: è´Ÿè½½ä¸å‡è¡¡,ä¸“å®¶å´©å¡Œ
# - å¤ªå¤§: å½±å“ä¸»ä»»åŠ¡æ€§èƒ½
moe_aux_loss_weight = 0.01  # æ¨èèŒƒå›´: 0.001 - 0.1

# Top-K é€‰æ‹©
# - K=1: æœ€ç¨€ç–,ä½†å¯èƒ½å®¹é‡ä¸è¶³
# - K=2: å¹³è¡¡é€‰æ‹© (æ¨è)
# - K>2: æ›´å¤šå®¹é‡,ä½†è®¡ç®—å¢åŠ 
top_k = 2
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits = model(input_ids)
    loss = compute_loss(logits, labels) + model.get_aux_loss()

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. è®¡ç®—ä¼˜åŒ–

```python
# å‡å°‘ä¸“å®¶æ•°é‡ä½†å¢åŠ å®¹é‡
# æ–¹æ¡ˆA: 8ä¸“å®¶ Ã— 2048 FFN = 16,384 å‚æ•°å•å…ƒ
# æ–¹æ¡ˆB: 4ä¸“å®¶ Ã— 4096 FFN = 16,384 å‚æ•°å•å…ƒ
# æ–¹æ¡ˆBè®¡ç®—æ›´å¿«ä½†ä¸“ä¸šåŒ–ç¨‹åº¦ä½
```

### 3. æ··åˆæ¶æ„ç­–ç•¥

```python
# DeepSeek V2 é£æ ¼: æµ…å±‚ç”¨æ ‡å‡†FFN,æ·±å±‚ç”¨MoE
n_layer = 32
moe_layers = list(range(16, 32))  # ååŠéƒ¨åˆ†ç”¨MoE

model = HybridMoETransformerLM(
    n_layer=n_layer,
    moe_layer_indices=moe_layers,
    ...
)
```


## ğŸ§ª å®éªŒå»ºè®®

### 1. æ¶ˆèå®éªŒ

```python
# æµ‹è¯•ä¸“å®¶æ•°é‡çš„å½±å“
for n_experts in [4, 8, 16, 32]:
    model = MoETransformerLM(n_experts=n_experts, ...)
    # è®­ç»ƒå’Œè¯„ä¼°
```

### 2. Top-K å®éªŒ

```python
# æµ‹è¯•æ¿€æ´»ä¸“å®¶æ•°çš„å½±å“
for top_k in [1, 2, 4]:
    model = MoETransformerLM(top_k=top_k, ...)
    # å¯¹æ¯”æ€§èƒ½å’Œè®¡ç®—æˆæœ¬
```

### 3. æ··åˆç­–ç•¥

```python
# æµ‹è¯•ä¸åŒçš„MoEå±‚åˆ†å¸ƒ
strategies = {
    "all": list(range(12)),          # æ‰€æœ‰å±‚
    "deep": list(range(6, 12)),      # æ·±å±‚
    "sparse": [2, 5, 8, 11],         # ç¨€ç–åˆ†å¸ƒ
}
```

## ğŸ“ æœ€ä½³å®è·µ

1. **ä»å°è§„æ¨¡å¼€å§‹**: å…ˆç”¨ 4-8 ä¸ªä¸“å®¶éªŒè¯æƒ³æ³•
2. **ç›‘æ§è´Ÿè½½**: ç¡®ä¿ä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨
3. **è°ƒæ•´è¾…åŠ©æŸå¤±**: æ ¹æ®ä»»åŠ¡è°ƒæ•´æƒé‡
4. **ä½¿ç”¨æ··åˆæ¶æ„**: ä¸æ˜¯æ‰€æœ‰å±‚éƒ½éœ€è¦ MoE
5. **æ¢¯åº¦è£å‰ª**: MoE è®­ç»ƒå¯èƒ½ä¸ç¨³å®š,ä½¿ç”¨æ¢¯åº¦è£å‰ª

## ğŸ”— å‚è€ƒèµ„æ–™

- [Shazeer et al. 2017 - Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer](https://arxiv.org/abs/1701.06538)
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)
- [Switch Transformers](https://arxiv.org/abs/2101.03961)

## ğŸ“„ è®¸å¯

æœ¬å®ç°ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

---

**ä½œè€…**: Which_W 

