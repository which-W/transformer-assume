{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9434698-3dab-458f-bad3-a9be43b3431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "输入词序列，先做id向量化,再给id附加位置信息\n",
    "'''\n",
    "from torch import nn \n",
    "import torch \n",
    "from dataset import de_vocab,de_preprocess,train_dataset\n",
    "import math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545eab1-a2f6-47ca-ab7d-e4423422fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPosition(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_size,dropout=0.1,seq_max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # 序列中的每个词id转emb向量, 其他形状不变\n",
    "        self.seq_emb=nn.Embedding(vocab_size,emb_size)\n",
    "\n",
    "        # 为序列中每个位置准备一个位置向量，也是emb_size宽\n",
    "        position_idx=torch.arange(0,seq_max_len,dtype=torch.float).unsqueeze(-1)\n",
    "        #获取每个位置向量与频率的乘积\n",
    "        position_emb_fill=position_idx*torch.exp(-torch.arange(0,emb_size,2)*math.log(10000.0)/emb_size)\n",
    "        #让每个词id序列等长\n",
    "        pos_encoding=torch.zeros(seq_max_len,emb_size)\n",
    "        #使用sin与cos来实现PE(pos + k) 可以通过一个 只依赖 k的线性变换得到。\n",
    "        pos_encoding[:,0::2]=torch.sin(position_emb_fill)\n",
    "        pos_encoding[:,1::2]=torch.cos(position_emb_fill)\n",
    "        self.register_buffer('pos_encoding',pos_encoding) # 固定参数,不需要train\n",
    "\n",
    "        # 防过拟合\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):    # x: (batch_size,seq_len)\n",
    "        x=self.seq_emb(x)   # x: (batch_size,seq_len,emb_size)\n",
    "        x=x+self.pos_encoding.unsqueeze(0)[:,:x.size()[1],:] # x: (batch_size,seq_len,emb_size) 这里的pos_encoding:(max_seq_size,emb_size)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0835a501-0f8f-4a1b-bf3d-0753c208cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_ids_tensor: torch.Size([8]) emb_result: torch.Size([1, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    emb=EmbeddingWithPosition(len(de_vocab),128)\n",
    "\n",
    "    de_tokens,de_ids=de_preprocess(train_dataset[0][0]) # 取de句子转词ID序列\n",
    "    de_ids_tensor=torch.tensor(de_ids,dtype=torch.long)\n",
    "\n",
    "    emb_result=emb(de_ids_tensor.unsqueeze(0)) # 转batch再输入模型\n",
    "    print('de_ids_tensor:', de_ids_tensor.size(), 'emb_result:', emb_result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ba2f9-487d-494f-a156-cb9f6503351e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
